# Copyright (C) 2025 Jean-Marc DIGNE
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
Logique pour l'int√©gration de la recherche de livres par ISBN sur Amazon.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re


def get_book_info_from_amazon(isbn, region="fr"):
    """
    R√©cup√®re les m√©tadonn√©es d'un livre Amazon via ISBN.
    :param isbn: ISBN-13 sans tirets (ex: '9782743664060')
    :param region: 'fr' pour amazon.fr, 'com' pour amazon.com, etc.
    :return: JSON avec m√©tadonn√©es
    """
    # Nettoie l'ISBN
    isbn = re.sub(r"[^0-9]", "", isbn)
    if len(isbn) not in (10, 13):
        return json.dumps({"error": "ISBN invalide"}, ensure_ascii=False, indent=4)

    # URL de recherche Amazon
    domain = f"amazon.{region}"
    search_url = f"https://www.{domain}/s?k={isbn}&i=stripbooks"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
        "Referer": f"https://www.{domain}/",
    }

    session = requests.Session()
    session.headers.update(headers)

    try:
        # √âtape 1: Recherche
        # print(f"Recherche sur {search_url}")
        time.sleep(1)  # D√©lai politeness
        response = session.get(search_url, timeout=10)
        response.raise_for_status()

        # Parse le HTML
        soup = BeautifulSoup(response.content, "lxml")

        # V√©rifie si la page contient un CAPTCHA
        if soup.select_one('form[action="/errors/validateCaptcha"]'):
            return json.dumps(
                {"error": "CAPTCHA d√©tect√©, requ√™te bloqu√©e par Amazon"},
                ensure_ascii=False,
                indent=4,
            )

        # Trouve les liens de produits
        product_links = soup.select(
            'a.a-link-normal.s-underline-text.s-underline-link-text.s-link-style, a.a-link-normal[href*="/dp/"]'
        )
        product_url = None
        if product_links:
            # S√©lectionne le lien pertinent
            for link in product_links:
                href = link.get("href", "")
                asin_match = re.search(r"/dp/([A-Z0-9]{10})", href)
                if asin_match:
                    asin = asin_match.group(1)
                    product_url = f"https://www.{domain}/dp/{asin}"
                    print(f"üìö Amazon ISBN:Lien produit s√©lectionn√©: {product_url}")
                    break

        if not product_url:
            return json.dumps(
                {"error": "Aucun produit trouv√© pour cet ISBN"},
                ensure_ascii=False,
                indent=4,
            )

        # √âtape 2: Page produit
        print(f"üìö Amazon ISBN:Page produit: {product_url}")
        time.sleep(1)
        response = session.get(product_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "lxml")

        # Extraction des m√©tadonn√©es
        book_data = {}
        book_data["titre"] = (
            soup.select_one("#productTitle").get_text(strip=True)
            if soup.select_one("#productTitle")
            else "Inconnu"
        )
        authors = soup.select(
            "a.a-link-normal.contributorName, .author a.a-link-normal, span.author a"
        )
        book_data["auteur"] = (
            "; ".join([a.get_text(strip=True) for a in authors])
            if authors
            else "Inconnu"
        )

        # √âditeur
        pub_elem = soup.select_one(
            'div#rpi-attribute-book_details-publisher .rpi-attribute-value, span:-soup-contains("√âditeur") + span, td:-soup-contains("√âditeur") + td'
        )
        book_data["editeur"] = pub_elem.get_text(strip=True) if pub_elem else "Inconnu"

        # Date publication
        date_elem = soup.select_one(
            'div#rpi-attribute-book_details-publication_date .rpi-attribute-value, span:-soup-contains("Date") + span, td:-soup-contains("Date") + td'
        )
        book_data["date_publication"] = (
            date_elem.get_text(strip=True) if date_elem else "Inconnu"
        )

        # Description
        desc_elem = soup.select_one("#productDescription, #bookDescription_feature_div")
        if desc_elem:
            # Nettoyer le r√©sum√© pour enlever les "En lire plus" etc.
            for hidden_span in desc_elem.select("span[style*='display: none']"):
                hidden_span.decompose()
            raw_summary = desc_elem.get_text(strip=True)

            if len(raw_summary) > 500:
                # Tronquer √† la fin de la derni√®re phrase avant 500 caract√®res
                trunc_limit = 500
                last_period_index = raw_summary.rfind(".", 0, trunc_limit)
                if last_period_index != -1:
                    truncated_summary = raw_summary[: last_period_index + 1]
                else:
                    # Si pas de phrase, couper au dernier mot
                    last_space_index = raw_summary.rfind(" ", 0, trunc_limit)
                    truncated_summary = (
                        raw_summary[:last_space_index]
                        if last_space_index != -1
                        else raw_summary[:trunc_limit]
                    )
                book_data["resume"] = truncated_summary + "<br>En lire plus..."
            else:
                book_data["resume"] = raw_summary
        else:
            book_data["resume"] = "Non disponible"

        # Pages
        pages_elem = soup.select_one(
            'div#rpi-attribute-book_details-fiona_pages .rpi-attribute-value, span:-soup-contains("pages") + span, td:-soup-contains("pages") + td'
        )
        book_data["pages"] = (
            pages_elem.get_text(strip=True) if pages_elem else "Inconnu"
        )

        # Note (√©toiles)
        rating_elem = soup.select_one("#acrPopover + span .a-icon-alt")
        book_data["note"] = (
            rating_elem.get_text(strip=True) if rating_elem else "Inconnu"
        )

        book_data["isbn"] = isbn
        book_data["couverture_url"] = (
            soup.select_one("#imgBlkFront, #main-image-container img")["src"]
            if soup.select_one("#imgBlkFront, #main-image-container img")
            else "Non disponible"
        )
        book_data["product_url"] = product_url

        return json.dumps(book_data, ensure_ascii=False, indent=4)

    except requests.RequestException as e:
        return json.dumps(
            {"error": f"Erreur de requ√™te: {str(e)}"}, ensure_ascii=False, indent=4
        )
    except Exception as e:
        return json.dumps(
            {"error": f"Erreur inattendue: {str(e)}"}, ensure_ascii=False, indent=4
        )


def generate_book_markdown_fragment(book_data_json: str) -> str:
    """
    G√©n√®re un fragment Markdown pour afficher les m√©tadonn√©es du livre.
    :param book_data_json: JSON string contenant les m√©tadonn√©es
    :return: Fragment Markdown
    """
    try:
        book_data = json.loads(book_data_json)
        if "error" in book_data:
            return f"**Erreur lors de la r√©cup√©ration des informations du livre :** {book_data['error']}"

        title = book_data.get("titre", "Inconnu")
        cover_url = book_data.get("couverture_url", "https://via.placeholder.com/150")
        author = book_data.get("auteur", "Inconnu")
        publisher = book_data.get("editeur", "Inconnu")
        pub_date = book_data.get("date_publication", "Inconnu")
        isbn = book_data.get("isbn", "Inconnu")
        pages = book_data.get("pages", "Inconnu")
        note = book_data.get("note", "Inconnu")
        product_url = book_data.get("product_url", "#")

        # Handle summary and "En lire plus..." link
        raw_resume = book_data.get("resume", "Non disponible")
        resume_text = raw_resume
        resume_more_link_markdown = ""
        if "<br>En lire plus..." in raw_resume:
            parts = raw_resume.split("<br>En lire plus...")
            resume_text = parts[0].strip()
            if product_url != "#":
                resume_more_link_markdown = f"[En lire plus...]({product_url})"

        markdown_fragment = f"""
@@Book **{title} de {author}**

![Couverture]({cover_url})

**Auteur :** {author}  
**√âditeur :** {publisher}  
**Date de publication :** {pub_date}  
**ISBN :** {isbn}  
**Pages :** {pages}  
**Note :** {note}  

**R√©sum√© :** {resume_text}  
{resume_more_link_markdown}

[Voir sur Amazon.fr]({product_url})
"""
        return markdown_fragment.strip()

    except json.JSONDecodeError:
        return "**Erreur :** Erreur lors du traitement des donn√©es JSON du livre."
    except Exception as e:
        return f"**Erreur inattendue lors de la g√©n√©ration du Markdown :** {e}"
